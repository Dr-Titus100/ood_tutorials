{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a46ebd1-761c-42e0-8e5d-5b5aad90a79f",
   "metadata": {},
   "source": [
    "## Benchmarking Machine Learning Code\n",
    "\n",
    "In this tutorial, we are going to tackle a common problem that most people often face while running their code on Borah or any other super computer out there. However, you don't have to worry, super computers are built to solve this kind of problem. We will perform a benchmarking test on some machine learning problem to demonstrate how we can take full advantage of the resources on Borah. \n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "- Let's assume you have a code that takes long hours to run. \n",
    "- In this case, let's say the code runs for at least 12 hours. \n",
    "- This kind of code might require more computing power than what is on your local computer. \n",
    "- So you decided to use Borah.\n",
    "- You always love to write and run or test your code in a Jupyter Notebook.\n",
    "- However, you only have 4 hours maximum when you Borah OnDemand and the code takes at least 12 hours to run.\n",
    "- What should you do?\n",
    "\n",
    "First of all Jupyter Notebooks are not ideal for running codes that run for several hours. As long as the code is still running, you will have to leave the notebook opened, the computer must always be on and/or always connected to the internet if the code requires an internet connection to run, etc. This means you have to basically babysit your code until it's done running. Jupyter Notebooks are good for writing and running/testing codes that do not require a lot of time to run. Let's say each cell in the notebook only takes a few seconds or minutes to run and in total it does not take several hours to run the entire notebook.\n",
    "\n",
    "In our example above, does that mean Borah cannot help? No. Borah can definitely help because it was built for tasks like this. This is why the people at the Research Computing Department are ther to help you. They are working hard every day just to make your experiecne on Borah easier and fun. This is also why this tutorial is made available to help you. Beyond this tutorial, you can still reach out to the Research Computing department for help on Borah and/or high performance computing related matters.\n",
    "\n",
    "### How to Solve the Problem\n",
    "\n",
    "Let's start by considering the sample code below.\n",
    "\n",
    "I have created a very large and deep PyTorch tutorial that:\n",
    "- Uses 10 million synthetic samples with 200 features.\n",
    "- Builds a 40-layer deep fully connected neural network with large hidden dimensions.\n",
    "- Trains for 50 epochs, which should take 12+ hours on a personal computer, especially without a high-end GPU.\n",
    "- Logs training loss and validation accuracy.\n",
    "\n",
    "\n",
    "Now, let's break down the complex PyTorch tutorial you just created. This code is designed to train a massive deep neural network on synthetic data, with the goal of maximizing training time and computational load for experimentation or benchmarking.\n",
    "\n",
    "---\n",
    "- **1. The code block:**\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "```\n",
    "    - Chooses GPU if available, otherwise defaults to CPU.\n",
    "    \n",
    "---\n",
    "- **2. The code block:**\n",
    "```python\n",
    "X, y = make_classification(n_samples=10_000_000, n_features=200, ...)\n",
    "```\n",
    "    - Generates 10 million samples with 200 features.\n",
    "    - 150 of the features are informative (helpful for classification). These are features that actually help in distinguishing between the classes (i.e. they influence the target `y`). \n",
    "    - 30 are redundant (correlated). This means they are linear combinations of the informative features — they’re correlated with the informative ones.\n",
    "    - The remaining 20 features are `noise features`. They're completely random and do not contribute to classification — they serve to make the problem more realistic by introducing irrelevant or noisy data. They help to test model robustness, simulate real-world data, which usually contains irrelevant or uninformative features, and prevent the model from overfitting to only clean, synthetic features.\n",
    "    \n",
    "---\n",
    "- **3. The code block:**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "```\n",
    "    - Splits the dataset into 90\\% training and 10\\% testing.\n",
    "\n",
    "---\n",
    "- **4. The code block:**\n",
    "```python\n",
    "class LargeDataset(Dataset):\n",
    "    ...\n",
    "```\n",
    "    - Is a custom data set class that wraps the NumPy arrays in a PyTorch data set for use with DataLoader.\n",
    "    \n",
    "---\n",
    "- **5. The code block:**\n",
    "```python\n",
    "train_loader = DataLoader(..., batch_size=2048, num_workers=4)\n",
    "```\n",
    "    - Creates Data Loaders and loads data in batches of 2048.\n",
    "    - `num_workers=4` means parallel data loading with 4 processes (if supported). Thus, it will take a longer time to load if not supported.\n",
    "    \n",
    "---\n",
    "- **6. The code block:**\n",
    "```python\n",
    "class DeepNN(nn.Module):\n",
    "    ...\n",
    "```\n",
    "    - Defines a massive deep neural network.\n",
    "    - This model is:\n",
    "        - 40 layers deep, each with:\n",
    "        - A Linear layer (fully connected)\n",
    "        - BatchNorm1d for training stability\n",
    "        - ReLU activation\n",
    "        - Dropout to prevent overfitting\n",
    "    - Final output: 2 classes (binary classification)\n",
    "    - This is what makes the model computationally expensive.\n",
    "    \n",
    "---\n",
    "- **7. The code block:**\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "```\n",
    "    - Defines loss and optimizer\n",
    "    - `CrossEntropyLoss` is suitable for classification tasks.\n",
    "    - `AdamW` is a variant of Adam that decouples weight decay (helps with regularization).\n",
    "    \n",
    "---\n",
    "- **8. The code block:**\n",
    "```python\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    ...\n",
    "```\n",
    "    - Is the training loop\n",
    "    - Each epoch:\n",
    "        - Loops over batches\n",
    "        - Sends inputs/labels to device\n",
    "        - Computes outputs --> loss --> gradients --> weight updates\n",
    "        - Tracks average loss per epoch\n",
    "     - Also includes validation at each epoch:\n",
    "        - Runs the model on the test set (no gradient tracking)\n",
    "        - Calculates and stores accuracy\n",
    "        \n",
    "---\n",
    "- **9. The code block:**\n",
    "```python\n",
    "start_time = time.time()\n",
    "...\n",
    "end_time = time.time()\n",
    "```\n",
    "    - Measures and prints how long training takes in hours.\n",
    "    \n",
    "---\n",
    "- **10. The code block:**\n",
    "```python\n",
    "plt.plot(train_losses, ...)\n",
    "```\n",
    "    - Saves line plots of:\n",
    "        - Training Loss vs. Epochs\n",
    "        - Validation Accuracy vs. Epochs    \n",
    "\n",
    "### Summary\n",
    "\n",
    "| Part | Description |\n",
    "|------|-------------|\n",
    "| Data | 10M samples, 200 features, binary classification |\n",
    "| Model | 40 hidden layers, each 2048 neurons deep |\n",
    "| Training | 50 epochs, large batches (2048), uses AdamW |\n",
    "| Runtime | Estimated 12+ hours on a personal CPU |\n",
    "| Output | Loss & accuracy plots + final test accuracy |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c567499-1ac2-473f-ba77-64f932208d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate a massive synthetic dataset (10 million samples, 200 features)\n",
    "X, y = make_classification(n_samples=10e6, n_features=200, n_informative=150,\n",
    "                           n_redundant=30, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Custom dataset class\n",
    "class LargeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = LargeDataset(X_train, y_train)\n",
    "test_dataset = LargeDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2048, shuffle=False, num_workers=4)\n",
    "\n",
    "# Very deep and wide neural network model\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(200, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            *[layer for _ in range(40) for layer in [\n",
    "                nn.Linear(2048, 2048),\n",
    "                nn.BatchNorm1d(2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ]],\n",
    "            nn.Linear(2048, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = DeepNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    val_accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Val Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {(end_time - start_time) / 3600:.2f} hours\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.savefig('loss_plot.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3ee819-5ca4-4a7b-8644-7b572b08f42c",
   "metadata": {},
   "source": [
    "### How to Improve the Code\n",
    "\n",
    "The following improvement can be made to the code.\n",
    "- Add TensorBoard logging.\n",
    "- Parallelize or optimize the data pipeline.\n",
    "- Turn this into a distributed training script \n",
    "- Save checkpoints for resuming later.\n",
    "\n",
    "Now, let's navigate through a step-by-step breakdown of how we improve the code using the suggested improvements above.\n",
    "\n",
    "---\n",
    "#### Add TensorBoard Logging.\n",
    "TensorBoard logging was added in the code. It appears inside the `train()` function and is scoped to `rank == 0`, so only one process (usually GPU 0) writes to TensorBoard to avoid duplication.\n",
    "\n",
    "---\n",
    "- **1. The code block:**\n",
    "```python\n",
    "log_dir = f\"runs/ddp_rank{rank}_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(log_dir=log_dir) if rank == 0 else None\n",
    "```\n",
    "    - Is the TensorBoard Setup (inside `train()`).\n",
    "    \n",
    "- **2. The code block:**\n",
    "```python\n",
    "if rank == 0:\n",
    "    writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/val\", accuracy, epoch)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Val Accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "    - Logging metrics after each epoch:\n",
    "    \n",
    "- **3. The code block:**\n",
    "```python\n",
    "if rank == 0:\n",
    "    writer.close()\n",
    "```\n",
    "    - Closes the writer after training\n",
    "    - What It Logs:\n",
    "        - `Loss/train`: The average training loss per epoch.\n",
    "        - `Accuracy/val`: The validation accuracy per epoch.\n",
    "    - You can visualize it by running the code below in terminal and then open your browser to [http://localhost:6006](http://localhost:6006).\n",
    "    ```bash\n",
    "    tensorboard --logdir=runs\n",
    "    ```\n",
    "\n",
    "#### Parallelize or Optimize the Data Pipeline.\n",
    "The data pipeline is now optimized for performance! Here's what was added:\n",
    "- Parallelization and performance enhancements:\n",
    "    - `num_workers=8`: Enables multi-process data loading.\n",
    "    - `pin_memory=True`: Speeds up data transfer to GPU.\n",
    "    - `prefetch_factor=4`: Loads data in advance to reduce I/O wait.\n",
    "    - `non_blocking=True`: Ensures faster GPU data transfers.\n",
    "    - `torch.set_num_threads(os.cpu_count())`: Fully utilizes available CPU cores.\n",
    "    - `PYTORCH_CUDA_ALLOC_CONF` set for better memory handling on CUDA.\n",
    "\n",
    "These changes should significantly improve data loading throughput and reduce GPU idle time.\n",
    "\n",
    "#### Turn this into a Distributed Training Script \n",
    "The script has now been fully converted for Distributed Data Parallel (DDP) training using PyTorch's `torch.distributed` module.\n",
    "- Key Features Added:\n",
    "    - Uses `torch.multiprocessing.spawn` to launch multiple processes across available GPUs.\n",
    "    - Leverages `DistributedSampler` to partition the dataset across processes.\n",
    "    - Wraps the model with `DistributedDataParallel` (DDP) for synchronized training.\n",
    "    - Logs metrics to TensorBoard only from rank 0 to avoid duplication.\n",
    "    - Ensures each epoch sees the full dataset using `train_sampler.set_epoch(epoch)`.\n",
    "\n",
    "#### Save Checkpoints for Resuming Later.\n",
    "- Checkpointing is now built into the distributed training script.\n",
    "- What was added:\n",
    "    - `save_checkpoint()`: Saves model, optimizer, and epoch state to a .pth file.\n",
    "    - `load_checkpoint()`: Loads from checkpoint if it exists, and resumes training.\n",
    "    - Per-rank checkpointing: Each process saves its own checkpoint (e.g., `checkpoint_rank0.pth`).\n",
    "- Usage:\n",
    "    - If you stop training, just re-run the script—it will pick up from the last saved epoch.\n",
    "    \n",
    "**Note:** Make sure you save the code below as a `.py` file. Also note that we eliminate the `matplotlib` plotting functions from this final code. This is because `matplotlib` will not work in a `.py` file. However, we can save the output to file so that we can plot them separately later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61336e98-6525-45a6-b309-6ff29191cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from sklearn.datasets import make_classification\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.set_num_threads(os.cpu_count())\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# Wrap the NumPy arrays in a PyTorch data set for use with DataLoader.\n",
    "class LargeDataset(Dataset): \n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Define a massive deep neural network.\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(200, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            *[layer for _ in range(40) for layer in [\n",
    "                nn.Linear(2048, 2048),\n",
    "                nn.BatchNorm1d(2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ]],\n",
    "            nn.Linear(2048, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def save_checkpoint(state, filename=\"checkpoint.pth\"):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename=\"checkpoint.pth\"):\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Loaded checkpoint from '{filename}' at epoch {start_epoch}\")\n",
    "        return start_epoch\n",
    "    else:\n",
    "        print(f\"No checkpoint found at '{filename}', starting from scratch.\")\n",
    "        return 0\n",
    "\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    device = torch.device(f\"cuda:{rank}\")\n",
    "\n",
    "    # Create 10 million synthetic dataset with 200 feature in total: 150 informative features and 30 redundant/correlated features. The remain. \n",
    "    X, y = make_classification(n_samples=10e6, n_features=200, n_informative=150,\n",
    "                               n_redundant=30, n_classes=2, random_state=42)\n",
    "    # Split the dataset into 90% training and 10% testing.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    train_dataset = LargeDataset(X_train, y_train)\n",
    "    test_dataset = LargeDataset(X_test, y_test)\n",
    "\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "    test_sampler = DistributedSampler(test_dataset, num_replicas=world_size, rank=rank, shuffle=False)\n",
    "\n",
    "    # Load data in batches of 2048. num_workers=4 means parallel data loading with 4 processes (if supported).\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2048, sampler=train_sampler,\n",
    "                              num_workers=8, pin_memory=True, prefetch_factor=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=2048, sampler=test_sampler,\n",
    "                             num_workers=8, pin_memory=True, prefetch_factor=4)\n",
    "\n",
    "    model = DeepNN().to(device)\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "    # Setup TensorBoard logging\n",
    "    log_dir = f\"runs/ddp_rank{rank}_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    writer = SummaryWriter(log_dir=log_dir) if rank == 0 else None\n",
    "\n",
    "    num_epochs = 50\n",
    "    checkpoint_file = f\"checkpoint_rank{rank}.pth\"\n",
    "    start_epoch = load_checkpoint(model, optimizer, checkpoint_file)\n",
    "\n",
    "    # Time measurement\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "\n",
    "        # TensorBoard logging step\n",
    "        if rank == 0:\n",
    "            writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "            writer.add_scalar(\"Accuracy/val\", accuracy, epoch)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Val Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "            # Save checkpoint\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.module.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, filename=checkpoint_file)\n",
    "\n",
    "    if rank == 0:\n",
    "        writer.close()\n",
    "        end_time = time.time()\n",
    "        print(f\"Training completed in {(end_time - start_time) / 3600:.2f} hours\")\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6601d6ac-a085-4d4d-af6a-8bf9945a5f51",
   "metadata": {},
   "source": [
    "### Submitting Jobs on Borah\n",
    "\n",
    "Since the code cannot run on our local machines or OnDemand, we will have to run it by submitting a job on Borah. Below is a sample script for submitting jobs on Borah. The codes below must be saved into a `.sh` file. \n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#Account and Email Information\n",
    "#SBATCH -A tnde  ## User ID\n",
    "#SBATCH --mail-type=end\n",
    "#SBATCH --mail-user=titusnyarkonde@u.boisestate.edu\n",
    "# Specify parition (queue)\n",
    "#SBATCH --partition=bsudfq\n",
    "# Join output and errors into output.\n",
    "#SBATCH -o test_borah.o%j\n",
    "#SBATCH -e test_borah.e%j\n",
    "# Specify job not to be rerunable.\n",
    "#SBATCH --no-requeue\n",
    "# Job Name.\n",
    "#SBATCH --job-name=\"test_borah_login2\"\n",
    "# Specify walltime.\n",
    "#SBATCH -t 06-23:59:59 \n",
    "# ###SBATCH --time=48:00:00\n",
    "# Specify number of requested nodes.\n",
    "#SBATCH -N 1\n",
    "# Specify the total number of requested procs:\n",
    "#SBATCH -n 48\n",
    "# number of cpus per task\n",
    "#SBATCH --cpus-per-task=1 \n",
    "# Number of GPUs per node.\n",
    "# #SBATCH --gres=gpu:1\n",
    "# load all necessary modules and ctivate the conda environment\n",
    "module load slurm\n",
    "module load gcc/7.5.0\n",
    "module load gsl/gcc8/2.6\n",
    "module load openmpi/gcc/64/1.10.7\n",
    "module load cuda11.0/toolkit/11.0.3 \n",
    "source /bsuhome/tnde/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate /bsuhome/tnde/miniconda3/envs/ml_tutorial\n",
    "# Echo commands to stdout (standard output).\n",
    "# set -x\n",
    "# Copy your code & data to your R2 Home directory using\n",
    "# the SFTP (secure file transfer protocol).\n",
    "# Go to the directory where the actual BATCH file is present.\n",
    "cd /bsuhome/tnde/ondemand/dev/ood_tutorials/template\n",
    " # The �python� command runs your python code.\n",
    "# All output is dumped into test_borah.o%j with �%j� replaced by the Job ID.\n",
    "## The file Multiprocessing.py must also \n",
    "## be in $/home/tnde/P1_Density_Calibration/Density3D\n",
    "mpirun -np 8 python3 ml_benchmarking.py >>log.out\n",
    "# python3 demo_analytic.py >>log.out\n",
    "```\n",
    "\n",
    "Now, we will run the code under different scenarios and report the time it took to finish running. Remember that this tutorial is about benchmarking the time the code takes to run. This means we are not interested in the model's performance and accuracy. We are only interested in the time it takes to finish running the code. Hence, we will not be showing training and validation accuracy/loss curves here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d53c2f-07b1-4d23-8e62-fd2b2613ad5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd954d1f-a2ed-49bc-8d4e-f5610a3633b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109de7bd-84bd-442c-82c9-edbcce8624cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31b237-f113-4522-befd-5d5d0881b730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c4769-3211-4da8-bd36-92e140db1880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_tutorial",
   "language": "python",
   "name": "ml_tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
